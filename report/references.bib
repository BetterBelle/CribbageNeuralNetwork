@InProceedings{adaptive_cribbage_player,
  author="Kendall, Graham
  and Shaw, Stephen",
  editor="Schaeffer, Jonathan
  and M{\"u}ller, Martin
  and Bj{\"o}rnsson, Yngvi",
  title="Investigation of an Adaptive Cribbage Player",
  booktitle="Computers and Games",
  year="2003",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="29--41",
  abstract="Cribbage is (normally) a two-player card game where the aim is to score 121 points before your opponent. The game has four stages, one of which involves discarding two cards from the six cards you are dealt. A later stage scores the four cards in your hand together with a card cut randomly from the deck after the discards have been made. The two cards that were discarded are used to form another hand, when combined with the two discards from your opponent. This additional hand is referred to as the crib or box and is scored alternatively by you and your opponent. In this work, we investigate how a strategy can be evolved that decides which cards should be discarded into the crib. Several methods are investigated with the best one being compared against a commercially available program.",
  isbn="978-3-540-40031-8"
}

@article{deepmind_2015,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}


@misc{cribbage_rules, 
    title={Learn to Play Cribbage}, 
    url={https://bicyclecards.com/how-to-play/cribbage/}, 
    journal={Cribbage}, 
    author={Bicycle Cards}, 
    year={2022}
}

@Inbook{reinforcement_learning,
  author="van Otterlo, Martijn
  and Wiering, Marco",
  title="Reinforcement Learning and Markov Decision Processes",
  bookTitle="Reinforcement Learning: State-of-the-Art",
  year="2012",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="3--42",
  abstract="Situated in between supervised learning and unsupervised learning, the paradigm of reinforcement learning deals with learning in sequential decision making problems in which there is limited feedback. This text introduces the intuitions and concepts behind Markov decision processes and two classes of algorithms for computing optimal behaviors: reinforcement learning and dynamic programming. First the formal framework of Markov decision process is defined, accompanied by the definition of value functions and policies. The main part of this text deals with introducing foundational classes of algorithms for learning optimal behaviors, based on various definitions of optimality with respect to the goal of learning sequential decisions. Additionally, it surveys efficient extensions of the foundational algorithms, differing mainly in the way feedback given by the environment is used to speed up learning, and in the way they concentrate on relevant parts of the problem. For both model-based and model-free settings these efficient extensions have shown useful in scaling up to larger problems.",
  isbn="978-3-642-27645-3",
  doi="10.1007/978-3-642-27645-3_1",
  url="https://doi.org/10.1007/978-3-642-27645-3_1"
}

@article{bellman,
  title={A Markovian decision process},
  author={Bellman, Richard},
  journal={Journal of mathematics and mechanics},
  pages={679--684},
  year={1957},
  publisher={JSTOR}
}

@article{temp_diff_analysis,
  title={An analysis of temporal-difference learning with function approximation},
  author={Van Roy, Benjamin and others},
  journal={Automatic Control, IEEE Transactions on},
  volume={42},
  number={5},
  pages={674--690},
  year={1997}
}

@misc{cribbage_rules_montana, 
  title={Six Card Cribbage}, 
  url={https://www.cs.montana.edu/users/paxton/cribbage.html}, 
  journal={Six Card Cribbage}, 
  author={Paxton, John}, 
  year={1995}
}

@misc{td_learning_cribbage, 
    title={Temporal Difference Reinforcement Learning Applied to Cribbage}, 
    url={http://r6.ca/cs486/},
    author={O'Connor, Russel}, 
    year={2000}
}

@article{drl_survey,  
  author={Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal={IEEE Signal Processing Magazine},   
  title={Deep Reinforcement Learning: A Brief Survey},   
  year={2017},  
  volume={34},  
  number={6},  
  pages={26-38},  
  doi={10.1109/MSP.2017.2743240}
}


@article{dropout,
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  year = {2014},
  issue_date = {January 2014},
  publisher = {JMLR.org},
  volume = {15},
  number = {1},
  issn = {1532-4435},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  journal = {J. Mach. Learn. Res.},
  month = {jan},
  pages = {1929â€“1958},
  numpages = {30},
  keywords = {neural networks, regularization, deep learning, model combination}
}
